{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vHwg12cSwpzf"},"outputs":[],"source":["import os, glob\n","\n","# The HOME of the datas of the project\n","HOME = \"/folder/data/\"\n","\n","# The Path to reach the directories\n","analyses = os.path.join(HOME+\"analyses\", \"\")\n","fileInterleave = os.path.join(HOME+\"fileinterleave\", \"\")\n","\n","# Reference genome\n","ref = os.path.join(HOME, \"reference_genome.txt\")\n","# resequenced genomes in interleave fastq\n","genomes = glob.glob(os.path.join(fileInterleave, \"*interleave.fastq\"))\n","\n","bwa = \"bwa\" # In the case of a bwa not in the path (more than one version for example)\n","samtools = \"samtools\"  # In the case of a samtools not in the path (more than one version for example)\n","bcftools = \"bcftools\"\n","\n","# Do index of the reference genome, it should be done only one time\n","#os.system(bwa+\" index -a bwtsw \"+ref)\n","\n","\n","\n","for genome in genomes:\n","    sam = os.path.join(analyses, os.path.split(genome.strip(\".fastq\")+\".sam\")[1])\n","    os.system(bwa+\" mem -t 2 -p \"+ref+\" \"+genome+\"  > \"+sam) # do the mapping\n","\n","    bam = sam.strip(\".sam\")+\".bam\"\n","    os.system(samtools +\" view -bS -q 25 -m 90 -f 2 -o \"+bam+\" \"+sam) # filter mapping and convert output in bam format. Only keep reads with al least 90 base perfectly aligned (option -m)\n","\n","    sortedBam = bam.strip(\".bam\")+\".sorted\"\n","    os.system(samtools+\" sort -@ 2 \"+bam+\" \"+sortedBam) # sort the bam mapping file\n","\n","    BCF = sortedBam.strip(\".sorted\")+\".bcf\"\n","    os.system(samtools+\" mpileup -t DP -t SP -uf \"+ref+\" \"+sortedBam+\".bam > \"+BCF) # create mpileup file for variant calling\n","\n","    BCF2 = BCF.strip(\".bcf\")+\".raw.bcf\"\n","    os.system(bcftools+\" call -O b -vm -o \"+BCF2+\" \"+BCF) #call variant\n","\n","    VCF = BCF2.strip(\".bcf\")+\".vcf\"\n","    os.system(bcftools+\" view \"+BCF2+\" | vcfutils.pl varFilter -Q 30 -d 10 > \"+VCF) # filter snps for quality and depth\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IoYb_CJqwpzh"},"outputs":[],"source":["# The HOME of the datas of the project\n","HOME = \"/folder/data/\"\n","\n","# The Path to reach the directories\n","analyses = os.path.join(HOME+\"analyses\", \"\")\n","\n","#filter vcf for only snps\n","\n","import os,glob\n","\n","var = glob.glob(os.path.join(analyses, \"*.vcf\"))\n","for elt in var:\n","    VCFsnps = elt.strip(\".vcf\")+\".snps.vcf\"\n","    cmd = \"grep -v \\\"INDEL\\\" \"+elt+\" > \"+VCFsnps+\" \"\n","    os.system(cmd)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmJOEv1rwpzh"},"outputs":[],"source":["# filter vcf file to maintain only homozygote variants\n","\n","lis = glob.glob(os.path.join(analyses, \"*.snps.vcf\"))\n","\n","for elem in lis:\n","    VCFsnpshomo = elem.strip(\".vcf\")+\".homo.vcf\"\n","    o = open(VCFsnpshomo, \"w\")\n","    for line in open(elem):\n","        if len(line.split(\"\\t\")) < 5: o.write(line)\n","        elif \",\" not in line.split(\"\\t\")[4]:\n","            o.write(line)\n","    o.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"siR6f8h4wpzi"},"outputs":[],"source":["# Compare CLC and Samtools SNPs using bedtools intersect\n","\n","\n","clc_result =  os.path.join(HOME, \"CLC\") # the clc vcf files need to be stored in a directory called CLC\n","samtools_results = os.path.join(HOME, \"analyses\") #path to the samtools vcf files\n","\n","genomes = glob.glob(os.path.join(samtools_results, \"*.snps.homo.vcf\"))\n","\n","for gen in genomes:\n","    id = gen.split(\".\")[0].split(\"/\")[-1]\n","    print id\n","    clc = os.path.join(clc_result, \"\"+id+\".clc.vcf\") # the name of the clc vcf files need to be: isolate.clc.vcf\n","    intersect = os.path.join(samtools_results, \"\"+id+\".intersect.vcf\")\n","    os.system(\"bedtools intersect -a \"+gen+\" -b \"+clc+\" > \"+intersect) # generate vcf intersect file\n","\n","    with open(intersect, \"r+\") as f:     # add format vcf to the head of the file for bedtools intersect\n","     old = f.read()\n","     f.seek(0)\n","     f.write(\"##fileformat=VCFv4.2\\n\" + old)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hHxqvLVUwpzj"},"outputs":[],"source":["# Annotations of snps with snpEff\n","\n","# annotate SNP and keep only SNP in genes\n","# Do not show DOWNSTREAM changes\n","# -no-intergenic    Do not show INTERGENIC changes\n","# -no-intron        Do not show INTRON changes\n","# -no-upstream        Do not show UPSTREAM changes\n","# -no-utr        Do not show 5_PRIME_UTR or 3_PRIME_UTR changes\n","\n","import os\n","\n","HOME = \"/folder/data/\"\n","\n","samtools_results = os.path.join(HOME, \"analyses\") #path to the samtools vcf files\n","annotedFiles = os.path.join(HOME, \"annotatedfiles\")\n","RE = os.path.join(annotedFiles, \"RE.bed\")\n","intersect = glob.glob(os.path.join(samtools_results, \"*.intersect.vcf\"))\n","\n","\n","\n","for gen in intersect:\n","    id = gen.split(\".\")[0].split(\"/\")[-1]\n","    eff = gen.strip(\".vcf\")+\".eff.vcf\"\n","    cmd = \"java -Xmx4g -jar /folder/snpEff/snpEff.jar ann -no-downstream -no-upstream -interval \"+RE+\" -v Oidma1 \"+gen+\" > \"+eff+\" \"\n","    os.system(cmd)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8wnCqHPqwpzj"},"outputs":[],"source":["%%bash\n","\n","# Table for general stats mapping and variants\n","\n","\n","#cd /folder/data/fileInterleave\n","\n","\n","#for ELEM in *interleave.fastq\n","#do\n","#    echo $ELEM\n","#    echo \"Number of reads\"\n","    # Each read of a fastq have 4 lines so we divide by 4 to have the number of reads\n","#    echo $(wc -l $ELEM) | awk '{split($0,a,\" \"); print a[1] / 4'} | bc\n","#print echo\n","\n","#done\n","\n","# count number of mapped reads\n","#cd /folder/data\n","\n","\n","for ELEM in *sorted.bam\n","# for ELEM in *interleave.bam\n","do\n","    echo \"Number of mapped reads: \" $ELEM\n","\n","    samtools view -c $ELEM\n","\n","  print echo\n","\n","done\n","\n","# count number of variants\n","for ELEM in *intersect.vcf\n","for ELEM in *raw.vcf\n","do\n","    echo \"Number of variant:\" $ELEM\n","    grep -v \"#\" $ELEM | wc -l\n","    #echo \"Number of SNPs\"$ELEM\n","    #grep -v \"#\" $ELEM | grep -v \"INDEL\" | wc -l\n","    #echo \"Number of INDEL\"$ELEM\n","    #grep -v \"#\" $ELEM | grep -c \"INDEL\"\n","\n","done\n","\n","# count number of SNPs homozygote\n","for ELEM in *snps.homo.vcf\n","do\n","    echo \"Number of homo SNPs:\" $ELEM\n","    grep -v \"#\" $ELEM | wc -l\n","\n","\n","\n","done\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1faBOB-fwpzj"},"outputs":[],"source":["import os, glob\n","\n","\n","##INFO=<ID=ANN,Number=.,Type=String,Description=\"Functional annotations: 'Allele | Annotation | Annotation_Impact | Gene_Name | Gene_ID | Feature_Type | Feature_ID | Transcript_BioType | Rank | HGVS.c | HGVS.p | cDNA.pos / cDNA.length | CDS.pos / CDS.length | AA.pos / AA.length | Distance | ERRORS / WARNINGS / INFO' \">\n","\n","\n","analyses = \"folder/data/analyses\"\n","\n","eff = glob.glob(os.path.join(analyses, \"*.eff.vcf\"))\n","\n","#java -jar folder/data/snpEff/SnpSift.jar filter \"ANN[*].BIOTYPE has 'non_coding'\" Zn.intersect.eff.vcf > Zn.non_coding.vcf\n","\n","\n","for gen in eff:\n","    id = gen.split(\".\")[0].split(\"/\")[-1]\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'custom'\" \"\"\" #count RE\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'intergenic_region'\" \"\"\" #count intergenic_region\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'missense_variant'\" \"\"\" #count missense_variant\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'synonymous_variant'\" \"\"\" #count synonymous_variant\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'stop_gained'\" \"\"\" #count stop_gained\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'start_lost'\" \"\"\" #count start_lost\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'stop_lost'\" \"\"\" #count stop_lost\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'intron_variant'\" \"\"\" #count intron_variant\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has '5_prime_UTR_variant'\" \"\"\" #count 5_prime_UTR_variant\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has '3_prime_UTR_variant'\" \"\"\" #count 3_prime_UTR_variant\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'splice_region_variant'\" \"\"\" #count splice_region_variant\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].FEATURE has 'transcript'\" \"\"\" #count transcript\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].BIOTYPE has 'Coding'\" \"\"\" #count Coding\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].IMPACT has 'HIGH'\" \"\"\" #count high impact variants\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].IMPACT has 'MODERATE'\" \"\"\" #count moderate impact variants\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].IMPACT has 'MODIFIER'\" \"\"\" #count modifier impact variants\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].IMPACT has 'LOW'\" \"\"\" #count low impact variants\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ERRORS has 'WARNING_TRANSCRIPT_NO_STOP_CODON'\" \"\"\" #count errors on stop codons\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ERRORS has 'WARNING_TRANSCRIPT_NO_START_CODON'\" \"\"\" #count errors on start codons\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ERRORS has 'WARNING_TRANSCRIPT_INCOMPLETE'\" \"\"\" #count errors on transcription\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"(ANN[*].ANNOTATION has 'splice_region_variant') ! (ANN[*].ANNOTATION has 'intron') \" \"\"\" #count splice_region_variant NON intron\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"(ANN[*].GENEID has 'gene_6860') && (ANN[*].FEATURE has 'transcript') \" \"\"\" #count  transcript gene_6860 Cu/Zn_SOD\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"(ANN[*].GENE has 'jgi.p_Oidma1_18612') && (ANN[*].ANNOTATION has '3_prime_UTR_variant' ) \" \"\"\" #count 3'UTR gene_6860\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"(ANN[*].GENE has 'jgi.p_Oidma1_18612') && (ANN[*].ANNOTATION has '5_prime_UTR_variant' ) \" \"\"\" #count 5'UTR gene_6860\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"(ANN[*].GENE has 'jgi.p_Oidma1_18612') && (ANN[*].ANNOTATION has 'intron_variant' ) \" \"\"\" #count introns gene_6860\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"(ANN[*].ANNOTATION has 'intergenic_region') && (ANN[*].GENE has 'jgi.p_Oidma1_18612-jgi.p_Oidma1_53214') \" \"\"\" #count 5' intergenic_region of gene 6860\n","    fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"(ANN[*].ANNOTATION has 'intergenic_region') && (ANN[*].GENE has 'jgi.p_Oidma1_103006-jgi.p_Oidma1_18612') \" \"\"\" #count 3' intergenic_region of gene 6860\n","    cmd = \" \"+fct+\" \"+gen+\" | grep -v \\\"#\\\" | wc -l\"\n","\n","    os.system(cmd)\n","    print id\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"oxe1XWzewpzk"},"outputs":[],"source":["#count snps common to all isolate and total number of snps (with no duplicate)\n","\n","#importa tutti i path necessari e definisce le variabili\n","import re,glob, os, sys\n","\n","\n","HOME = \"/folder/data/\"\n","analyses = os.path.join(HOME+\"analyses\", \"\")\n","joinedVcf = os.path.join(analyses, \"vcfGenomes\")\n","\n","snps = glob.glob(os.path.join(analyses, \"*.intersect.vcf\"))\n","\n","\n","dictSet={}\n","dictotal = {}\n","\n","#from the files .intersect.vcf of each isolate, find all the SNPs and identify its position (scaffoldID+position)\n","#generate a \"dicSect\" per each isolate\n","for isolate in snps:\n","    id = isolate.split(\"/\")[-1].split(\".\")[0]\n","    dictSet[id]=set()\n","    lireFile = open(isolate)\n","    for line in lireFile.readlines():\n","        if \"#\" not in line:\n","            elts = line.split(\"\\t\")\n","            scaffId= elts[0].split()[0]\n","            dictSet[id].add(scaffId+\"-\"+elts[1])\n","            pos = scaffId+\"-\"+elts[1]\n","            dictSet[id].add(pos)\n","            dictotal[pos] = line\n","\n","#make intersection and union of all the dictSect and generate two files, respectively \"partage\" and \"ensemble\"\n","partage = set.intersection(dictSet[\"4H1\"],dictSet[\"5L3\"],dictSet[\"A\"],dictSet[\"E\"], dictSet[\"91\"], dictSet[\"534\"], dictSet[\"505\"], dictSet[\"506\"], dictSet[\"518\"], dictSet[\"523\"], dictSet[\"539\"], dictSet[\"13G\"], dictSet[\"4E\"], dictSet[\"1354\"], dictSet[\"1357\"], dictSet[\"1358\"])\n","ensemble = set.union(dictSet[\"Cd\"], dictSet[\"4H1\"],dictSet[\"5L3\"],dictSet[\"A\"],dictSet[\"E\"], dictSet[\"91\"], dictSet[\"534\"], dictSet[\"505\"], dictSet[\"506\"], dictSet[\"518\"], dictSet[\"523\"], dictSet[\"539\"], dictSet[\"13G\"], dictSet[\"4E\"], dictSet[\"1354\"], dictSet[\"1357\"], dictSet[\"1358\"])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uUqyTq6Nwpzk"},"outputs":[],"source":["a = len(dictotal)\n","b= len(ensemble)\n","c= len(partage)\n","print a, b, c"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vx1bpEjowpzl"},"outputs":[],"source":["#script improved (more rapid) to obtain the two files with shared (partage) and total (union) SNPs.\n","\n","HOME = \"/folder/data/\"\n","analyses = os.path.join(HOME+\"analyses\", \"\")\n","joinedVcf = os.path.join(analyses, \"vcfGenomes\")\n","\n","outfile = os.path.join(joinedVcf, \"common.snps.vcf\")\n","o = open(outfile, \"w\")\n","\n","#for each key and value of the dictionary \"dictotal\" and for each element in \"partage\" if a correspondence is found, write the values in the output file.\n","for elt in partage:\n","    a = dictotal.get(elt)\n","    o.write(a)\n","\n","o.close()\n","\n","\n","\n","\n","#######associates to the \"enseble\" file with the SNPs their respective annotation\n","\n","outfile = os.path.join(joinedVcf, \"all.snps.vcf\")\n","o = open(outfile, \"w\")\n","\n","#for each key and value of the dictionary \"dictotal\" and for each element in \"partage\" if a correspondence is found, write the values in the output file.\n","\n","for elt in ensemble:\n","    a = dictotal.get(elt)\n","    o.write(a)\n","\n","o.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OsCsqAVfwpzm"},"outputs":[],"source":["# Annotations of  all (from \"ensemble\")  and common (from \"partage\") snps with snpEff\n","\n","# annotate SNP and keep only SNP in genes\n","# Do not show DOWNSTREAM changes\n","# -no-intergenic    Do not show INTERGENIC changes\n","# -no-intron        Do not show INTRON changes\n","# -no-upstream        Do not show UPSTREAM changes\n","# -no-utr        Do not show 5_PRIME_UTR or 3_PRIME_UTR changes\n","\n","import os, glob\n","\n","HOME = \"/folder/data/\"\n","\n","samtools_results = os.path.join(HOME, \"analyses/vcfGenomes\") #path to the samtools vcf files\n","annotedFiles = os.path.join(HOME, \"annotatedfiles\")\n","RE = os.path.join(annotedFiles, \"RE.bed\")\n","all_common = glob.glob(os.path.join(samtools_results, \"*.snps.vcf\"))\n","\n","for gen in all_common:\n","    id = gen.split(\".\")[0].split(\"/\")[-1]\n","    print id\n","    eff = gen.strip(\".snps.vcf\")+\".eff.vcf\"\n","    cmd = \"java -Xmx4g -jar /folder/data/snpEff/snpEff.jar ann -no-downstream -no-upstream -interval \"+RE+\" -v Oidma1 \"+gen+\" > \"+eff+\" \"\n","    os.system(cmd)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oRH0vGTjwpzn"},"outputs":[],"source":["#mappare SNPs totali  e comuni nel genoma\n","\n","import os, glob\n","\n","\n","##INFO=<ID=ANN,Number=.,Type=String,Description=\"Functional annotations: 'Allele | Annotation | Annotation_Impact | Gene_Name | Gene_ID | Feature_Type | Feature_ID | Transcript_BioType | Rank | HGVS.c | HGVS.p | cDNA.pos / cDNA.length | CDS.pos / CDS.length | AA.pos / AA.length | Distance | ERRORS / WARNINGS / INFO' \">\n","\n","\n","HOME = \"/folder/data/\"\n","\n","eff = glob.glob(os.path.join(HOME, \"*.eff.vcf\"))\n","\n","#java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].BIOTYPE has 'non_coding'\" Zn.intersect.eff.vcf > Zn.non_coding.vcf\n","\n","\n","for gen in eff:\n","    id = gen.split(\".\")[0].split(\"/\")[-1]\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'custom'\" \"\"\" #count RE\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'intergenic_region'\" \"\"\" #count intergenic_region\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'missense_variant'\" \"\"\" #count missense_variant\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'synonymous_variant'\" \"\"\" #count synonymous_variant\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'stop_gained'\" \"\"\" #count stop_gained\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'start_lost'\" \"\"\" #count start_lost\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'stop_lost'\" \"\"\" #count stop_lost\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'intron_variant'\" \"\"\" #count intron_variant\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has '5_prime_UTR_variant'\" \"\"\" #count 5_prime_UTR_variant\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has '3_prime_UTR_variant'\" \"\"\" #count 3_prime_UTR_variant\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ANNOTATION has 'splice_region_variant'\" \"\"\" #count splice_region_variant\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].FEATURE has 'transcript'\" \"\"\" #count transcript\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].BIOTYPE has 'Coding'\" \"\"\" #count Coding\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].IMPACT has 'HIGH'\" \"\"\" #count high impact variants\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].IMPACT has 'MODERATE'\" \"\"\" #count moderate impact variants\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].IMPACT has 'MODIFIER'\" \"\"\" #count modifier impact variants\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].IMPACT has 'LOW'\" \"\"\" #count low impact variants\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ERRORS has 'WARNING_TRANSCRIPT_NO_STOP_CODON'\" \"\"\" #count errors on stop codons\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ERRORS has 'WARNING_TRANSCRIPT_NO_START_CODON'\" \"\"\" #count errors on start codons\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"ANN[*].ERRORS has 'WARNING_TRANSCRIPT_INCOMPLETE'\" \"\"\" #count errors on transcription\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"(ANN[*].ANNOTATION has 'splice_region_variant') ! (ANN[*].ANNOTATION has 'intron') \" \"\"\" #count splice_region_variant NON intron\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"(ANN[*].GENEID has 'gene_6860') && (ANN[*].FEATURE has 'transcript') \" \"\"\" #count  transcript gene_6860 Cu/Zn_SOD\n","    fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"(ANN[*].GENE has 'jgi.p_Oidma1_18612') && (ANN[*].ANNOTATION has '3_prime_UTR_variant' ) \" \"\"\" #count 3'UTR gene_6860\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"(ANN[*].GENE has 'jgi.p_Oidma1_18612') && (ANN[*].ANNOTATION has '5_prime_UTR_variant' ) \" \"\"\" #count 5'UTR gene_6860\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"(ANN[*].GENE has 'jgi.p_Oidma1_18612') && (ANN[*].ANNOTATION has 'intron_variant' ) \" \"\"\" #count introns gene_6860\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"(ANN[*].ANNOTATION has 'intergenic_region') && (ANN[*].GENE has 'jgi.p_Oidma1_18612-jgi.p_Oidma1_53214') \" \"\"\" #count 5' intergenic_region of gene 6860\n","    #fct = \"\"\"java -jar /folder/data/snpEff/SnpSift.jar filter \"(ANN[*].ANNOTATION has 'intergenic_region') && (ANN[*].GENE has 'jgi.p_Oidma1_103006-jgi.p_Oidma1_18612') \" \"\"\" #count 3' intergenic_region of gene 6860\n","    cmd = \" \"+fct+\" \"+gen+\" | grep -v \\\"#\\\" | wc -l\"\n","\n","    os.system(cmd)\n","    print id\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fdCtZvwZwpzn"},"outputs":[],"source":["# generate new file fasta for each genome considering snps\n","\n","\n","import os, glob\n","\n","#The HOME of the datas of the project\n","HOME = \"/folder/data/\"\n","\n","#The Path to reach the directories\n","analyses = os.path.join(HOME+\"analyses\", \"\")\n","\n","\n","# Reference genome\n","ref = os.path.join(HOME, \"ref_genome_Oidma_jgi/Oidma1.txt\")\n","\n","#vcf file location\n","#c = \"gunzip *.gz\"\n","#os.system(c)\n","#print c\n","\n","vcf = glob.glob(os.path.join(analyses, \"*.intersect.vcf\"))\n","\n","\n","\n","for gen in vcf:\n","  #  c = \"export PERL5LIB=/Users/cmurat/Downloads/vcftools_0.1.12b/perl\"\n","  #  os.system(c)\n","    id = gen.split(\"/\")[-1].split(\".\")[0]\n","    newFasta = os.path.join(analyses, \"\"+id+\".new.fasta\")\n","    bamfile = os.path.join(analyses, \"\"+id+\".interleave.sorted.bam\")\n","    perbaseLowCov = os.path.join(analyses, \"\"+id+\".perbaseLowCov.bed\")\n","    mask = os.path.join(analyses, \"\"+id+\".new.mask.fasta\")\n","    snpsFasta = os.path.join(analyses, \"toto.snps.fasta\")\n","\n","    cmd1 = \"bgzip \"+gen+\" ; tabix -p vcf \"+gen+\".gz\" # prepare the vcf file with bzip and tabix for vcftools\n","    cmd2 = \"cat \"+ref+\" | vcf-consensus \"+gen+\".gz > \"+newFasta+\"\" #generate the fasta file for each genome with snps\n","    cmd3 = \"bedtools genomecov -bga -ibam \"+bamfile+\" | awk '$4<10' > \"+perbaseLowCov+\"\" #generate bed file with interval with low coverage (in this case < 10 reads)\n","    cmd4 = \"bedtools maskfasta -mc X -fi \"+newFasta+\" -bed \"+perbaseLowCov+\" -fo \"+mask+\" \"\n","\n","\n","    print cmd1\n","    os.system(cmd1)\n","    print cmd2\n","    os.system(cmd2)\n","    print cmd3\n","    os.system(cmd3)\n","    print cmd4\n","    os.system(cmd4)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ErAvuzqewpzo"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"sCAMMOC_wpzo"},"outputs":[],"source":["#generate a unique fasta file with snps positions for all genomes using a sorted vcf file!\n","\n","import os, glob\n","\n","HOME = \"/folder/data/\"\n","analyses = os.path.join(HOME+\"analyses\", \"\")\n","\n","#vcf file location\n","vcf = glob.glob(os.path.join(analyses, \"*.intersect.vcf\"))\n","#vcf = glob.glob(os.path.join(analyses, \"505.intersect.vcf\")) ##example\n","total = os.path.join(analyses+\"/vcfGenomes\", \"all.snps.sorted.vcf\")\n","\n","outfile = os.path.join(analyses, \"Allsnps.sorted.fasta\")\n","#outfile = os.path.join(analyses, \"505snps.sorted.fasta\") ##example\n","\n","out = open(outfile, \"a\")\n","\n","for gen in vcf:\n","    id = gen.split(\"/\")[-1].split(\".\")[0]\n","    print gen\n","    #snpsFasta = os.path.join(analyses, \"\"+id+\".snps2.fasta\")\n","    snpsFasta = os.path.join(analyses, \"\"+id+\".snps.fasta\")\n","    mask = os.path.join(analyses, \"\"+id+\".new.mask.fasta\")\n","\n","    cmd1 = \"bedtools getfasta -fi \"+mask+\" -bed \"+total+\" -fo  \"+snpsFasta+\" \"\n","    #print cmd1\n","    os.system(cmd1)\n","    a = open(snpsFasta).readlines()\n","    print id\n","    out.write(\"\\n>\"+id+\"\\n\")\n","    for line in a:\n","        if \">\" not in line:\n","            base = line.strip()\n","            out.write(base)\n","out.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5QzuiPuwpzo","outputId":"363d400f-16db-4637-c2f4-040636cd62ee"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-23-0759dff019ef>, line 1)","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-0759dff019ef>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    usr(/bin/bash)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["/usr/bin/bash\n","\n","cd /folder/data/analyses/\n","\n","mkdir phylogeny\n","\n","cat Allsnps.fasta Zn.Allsnps.fasta > phylogeny/Allsnps.def.fasta"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3pVy0Bpywpzp"},"outputs":[],"source":["#count line length\n","import os\n","\n","HOME = \"folder/data/\"\n","analyses = os.path.join(HOME+\"analyses/phylogeny\", \"\")\n","\n","\n","outfile = open(os.path.join(analyses, \"Allsnps.def.fasta\"))\n","\n","for line in outfile:\n","    if \">\" not in line:\n","        print len(line)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7o-Qh2ECwpzp"},"outputs":[],"source":["#count bases\n","\n","import os\n","\n","HOME = \"folder/data/\"\n","analyses = os.path.join(HOME+\"analyses/\", \"\")\n","\n","outfile = open(os.path.join(analyses, \"5L3.new.mask.fasta\"))\n","i = 0\n","for line in outfile:\n","    if \">\" not in line:\n","        i = i + len(line)\n","\n","print i"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"09Nf-6j0wpzq"},"outputs":[],"source":["#command line for quicktree. Needs bioscript.convert\n","\n","quicktree -in a -out t -boot 100 Tuofileinstockolmformat > fileout.st.aln.100boot.nwk\n","\n","https://pypi.python.org/pypi/bioscripts.convert/0.4"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"WVRmsfjkwpzq"},"outputs":[],"source":["import sys,os,re\n","\n","from Bio import SeqIO\n","\n","HOME = \"/folder/data/analyses\"\n","\n","fileSeq = os.path.join(HOME, \"Allsnps.sorted.fasta\")\n","#fileSeq = os.path.join(HOME+\"/phylogeny/\", \"example.fasta\")\n","fileSeq = open(fileSeq,\"r\")\n","\n","outfile = os.path.join(HOME, \"Allsnps.sorted.pcadapt\")\n","#outfile = os.path.join(HOME+\"/phylogeny/\", \"example.pcadapt\")\n","outfile = open(outfile, \"w\")\n","\n","\n","for cur_record in SeqIO.parse(fileSeq, \"fasta\") :\n","    print cur_record.id\n","    if cur_record.id == \"Zn\":\n","        ref = cur_record.seq\n","    for i in range(len(cur_record.seq)):\n","        if cur_record.seq[i] == ref[i]:\n","            outfile.write(\"0 \")\n","\n","        elif cur_record.seq[i] == \"N\" or cur_record.seq[i] == \"X\":\n","            outfile.write(\"9 \")\n","\n","        else:\n","            outfile.write(\"1 \")\n","\n","    outfile.write(\"\\n\")\n","\n","outfile.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"W_0MCDCMwpzq"},"outputs":[],"source":["#export 10Kb windows in single files containing the same window of all isolates.\n","HOME = \"folder/data/\"\n","\n","import sys,os,re, glob\n","\n","from Bio import SeqIO\n","\n","c\n","\n","fileSeq = glob.glob(os.path.join(HOME, \"*10Kb.fasta\"))\n","\n","\n","for f in fileSeq:\n","    isolate = f.split(\".\")[0].split(\"/\")[-1]\n","\n","    f = open(f,\"r\")\n","\n","    for cur_record in SeqIO.parse(f, \"fasta\") :\n","\n","\n","        interval = cur_record.id\n","\n","        fileout = os.path.join(HOME, interval+\".fasta\")\n","        fileout = open(fileout, \"a\")\n","        fileout.write(\">\"+isolate+\"\\n\"+str(cur_record.seq)+\"\\n\")\n","        fileout.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"X_jniYcpwpzr"},"outputs":[],"source":["#export each gene in single files containing the sequence of the same gene of all isolates.\n","\n","HOME = \"folder/data/\"\n","import sys,os,re, glob\n","\n","from Bio import SeqIO\n","\n","c\n","\n","fileSeq = glob.glob(os.path.join(HOME, \"*genes.fasta\"))\n","\n","\n","for f in fileSeq:\n","    isolate = f.split(\".\")[0].split(\"/\")[-1]\n","\n","    f = open(f,\"r\")\n","\n","    for cur_record in SeqIO.parse(f, \"fasta\") :\n","\n","\n","        interval = cur_record.id\n","\n","        fileout = os.path.join(HOME, interval+\".fasta\")\n","        fileout = open(fileout, \"a\")\n","        fileout.write(\">\"+isolate+\"\\n\"+str(cur_record.seq)+\"\\n\")\n","        fileout.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dJQ3H69Dwpzr"},"outputs":[],"source":["#egglib check format\n","\n","import egglib\n","\n","HOME = \"folder/data/gene_windows\"\n","\n","align = egglib.Align(HOME+\"scaffold_1:25551-27296.fasta\", groups=True)\n","for name,sequence,group in align:\n","    print 'name:',name, 'group:',group\n"]},{"cell_type":"code","source":["#egglib 3.0.0b25 analyses: polymorphism of genes or 10Kb windows\n","#torino, OTTOBRE 2019\n","#analisi di polimorfismo sui geni\n","\n","\n","#####the analysis requires that in at least 6 isolates there are nucleotide in all the positions.\n","\n","\n","import sys,os,re,glob\n","import egglib\n","\n","\n","HOME = \"folder/data/\"\n","outfile = os.path.join(HOME+\"/genes_analyses\", \"stats.egglib_genes.txt\")\n","outfile = open(outfile, \"w\")\n","\n","outfile.write(\"Interval\\tNum_samples\\tNum_sites\\tS\\tTajima'sD\\tTheta\\tPi\\tFst\\tKst\\tGst\\tWCst\\n\")\n","\n","\n","#list of loci to analyse\n","loci = glob.glob(os.path.join(HOME+\"/gene_windows\", \"*.fasta\"))\n","#loci = glob.glob(os.path.join(HOME+\"/10Kbwindows\", \"*.fasta\"))\n","\n","\n","print len(loci)\n","\n","cs_all = egglib.stats.ComputeStats()\n","cs_all.add_stats('Fst', 'nseff', 'lseff', 'WCst', 'D', 'Pi', 'thetaW', 'D', 'S', 'Kst', 'Gst')\n","\n","cs10 = egglib.stats.ComputeStats()\n","cs10.add_stats('ns_site')\n","\n","cs20 = egglib.stats.ComputeStats()\n","cs20.add_stats('ns_site')\n","\n","# redo requiring >= MINI sample per population\n","MINI = 6\n","for fname in sorted(loci):\n","    aln = egglib.io.from_fasta(fname, labels=True, alphabet=egglib.alphabets.Alphabet('char', 'ACGT', 'XN-'))\n","    mapping = aln.group_mapping()\n","    struct_all = egglib.stats.get_structure(aln, lvl_pop=1, ploidy = 1)\n","    #print struct_all.as_dict()\n","\n","    # make two structure objects\n","    struct_10 = struct_all.as_dict()\n","    del struct_10[0][0][20]\n","    struct_10 = egglib.stats.make_structure(* struct_10)\n","    #print struct_10.as_dict()\n","    struct_20 = struct_all.as_dict()\n","    del struct_20[0][0][10]\n","    struct_20 = egglib.stats.make_structure(* struct_20)\n","    #print struct_20.as_dict()\n","\n","    cs_all.configure(struct=struct_all, multi=True)\n","    cs10.configure(struct=struct_10)\n","    cs20.configure(struct=struct_20)\n","\n","    # analyse only sites with enough samples in each pop\n","    for pos in xrange(aln.ls):\n","        site = egglib.stats.site_from_align(aln, pos)\n","        n10 = cs10.process_site(site)['ns_site']\n","        #print n10\n","        n20 = cs20.process_site(site)['ns_site']\n","        #print n20\n","        if n10 is not None and n10 >= MINI and n20 is not None and n20 >= MINI:\n","            cs_all.process_site(site)\n","\n","    stats = cs_all.results()\n","    #print fname, stats['nseff'], stats['lseff'], stats['S'], stats['D'], stats['thetaW']/stats['lseff'], stats['Pi']/stats['lseff'], stats['Fst'], stats['Kst'], stats['Gst'], stats['WCst']\n","    #print stats['S']\n","\n","\n","    # extracts statistics in table\n","    l= str(fname)\n","    interval = l.split(\"/\")[-1]\n","    a, b, c = re.match('scaffold_(\\d+)\\:(\\d+)\\-(\\d+)\\.fasta', interval).groups()\n","    interval = 'scaffold_{0}:{1}-{2}.fasta'.format(a.rjust(2, '0'), b.rjust(7, '0'), c.rjust(7, '0'))\n","    F = format(stats['Fst'])\n","    W = format(stats['WCst'])\n","    T = format(stats['D'])\n","    P = format(stats['Pi'])\n","    Th = format(stats['thetaW'])\n","    S = format(stats['S'])\n","    L = format(stats['lseff'])\n","    N = format(stats['nseff'])\n","    K = format(stats['Kst'])\n","    G = format(stats['Gst'])\n","\n","\n","\n","\n","    #print interval, F, T, P, Th\n","    outfile.write(interval+\"\\t\"+N+\"\\t\"+L+\"\\t\"+S+\"\\t\"+T+\"\\t\"+Th+\"\\t\"+P+\"\\t\"+F+\"\\t\"+K+\"\\t\"+G+\"\\t\"+W+\"\\n\")\n","\n","outfile.close()\n"],"metadata":{"id":"W5pFpuEMaurL"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}